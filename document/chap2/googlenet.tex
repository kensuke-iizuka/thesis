\chapter{GoogLeNet}
{
\label{chap:googlenet}
本研究でマルチFPGA上に実装するアプリケーションであるGoogLeNetについて説明する．
GoogleNetは畳込みニューラルネットワーク(CNN: Convolutional Neural Network)のモデルの1つである．
まずCNNの基本的な演算について説明する
\section{Convolutional Neural Network}
\label{sec:cnn}
2012年のILSVRC()で登場したAlexNetによりCNNは特に画像認識や物体検出の分野で優れた識別精度をマークしたことから世界で注目されるようになった，
CNNはその名前にも含まれているように式で表される畳み込み演算と呼ばれる行列の積和演算がその主なアルゴリズムである．
\begin{eqnarray}
	output(x, y)^{i} = input(x, y)^{i} / (k + \alpha \sum_{j=max(0, i-n/2)}^{min(N-1, i+n/2)} (a^j_{x, y})^2)^\beta
\end{eqnarray}
ニューラルネットワークは上述の畳込み演算に加えてプーリング，全結合層などの演算を層という形で積み重ねていき画像における画素値のような入力された行列が
各層を伝搬していき出力されるという構造をとっている．
これらの層において重みフィルターと呼ばれる行列で表されるパラメータと入力行列各要素について積和演算を行うため，CNNは計算量がとても多く，多量のパラメータを
利用することから多くのメモリアクセスを要する、非常にコンピュータ資源を必要とするアルゴリズムである．

\section{GoogLeNet}
\label{sec:googlenet}
GoogLeNetは2014年のILSVRCで最高精度をマークしたCNNのモデルの一つである．構成要素は上述の畳み込み演算，プーリング処理などが主であるという点においてはAlexNetなどと
違いはないが，それぞれの層の接続の仕方，層の深さが異なる，AlexNet, GoogLeNetそれぞれについて図にその全体像を示す

両者を比較すると，GoogLeNetのほうが，層がより深くなっていること，さらに横に広がっていることがわかる.
GoogLeNetは層を深くする代わりにそのフィルタサイズを小さくすることで，計算量，メモリアクセスを減らすように設計されている．
〇〇の研究によって一つの大きなフィルタによる畳込みよりも複数の小さなフィルタによる畳込みのほうがより高い精度を出すことができるとされている．
さらにGoogLeNetでは横に広がった層をInception層と名付け，これを複数層重ねる設計をしている．

\section{Inception}
\label{sec:inception}
Inception層は次の図で示される(これは図の一部を拡大したものと同じである)
これを見ると横に広がった層の中で1＊1convと記されている層がある.この層ではサイズ１のフィルターを用いて，畳込み演算を行っている.
この層は，次元削減を行っている．これは入力チャネル(入力行列の深さ)に対して少ない層のフィルタを畳み込み演算することでその深さを
削減する，これによって計算量が減るだけでなく，精度向上が実現できる．
Inception層の出力の手前のDepthConcat層は横に広がった層のそれぞれの出力結果を図○のように深さ方向に結合することで
一つの行列として出力値にまとめる処理を行っている．
この特徴的なInception層を積層していくことで，GoogLeNetは構成される．表○にそれぞれの層で必要なパラメータ数をまとめる．
この表を見るとAlecNetに見られる全結合層がないことがわかる．GoogLeNetでは高い認識精度を保ったまま，計算量の多い全結合層を
削除することで全体の計算量を減らしている．
}